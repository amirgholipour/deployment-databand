{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run the sample python pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a simple python program will read a small csv file and it will load it in a Postgres database. No Airflow and no other tool is neeeded for this program to run properly. It can be executed on your local laptop if you have a python environment and installed the necessary packages. Additionally, we will need to capture some parameters of our deployment on OpenShift and modify four lines of the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install python packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We need the following python packages in the sample pipeline:\n",
    "- `pandas`: to create a dataframe where the csv data will reside\n",
    "-  `psycopg2`: to access Postgres\n",
    "-  `sqlalchemy`: to move a pandas dataframe into Postgres without warnings or errors\n",
    "-  `dbnd`: to collect performance data that will be pushed to the Databand system \n",
    "\n",
    "Run the next cell to install these prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# This cell installs the packages needed to run pythondag.py\n",
    "pip install psycopg2-binary\n",
    "pip install pandas\n",
    "pip install sqlalchemy\n",
    "pip install dbnd\n",
    "# This last installation may not be necessary on your particular environment\n",
    "pip install more-itertools==9.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Retrieve deployment parameters for the python pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following parameters for our deployment:\n",
    "- The IP Address and the port of the Postgres service (route and nodeport)\n",
    "- The IP Address of the Databand service (route)\n",
    "- A personal API key to access the Databand service\n",
    "\n",
    "Let's begin with the Postgres parameters. We can retrieve them from the command line or by accessing the OpenShift interface. We will expore both ways in order to verify that produce the same results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start by logging to the cluster and selecting the Postgres project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the command with your own one inside the single quotes and run the cell\n",
    "# Example OC_LOGIN_COMMAND='oc login --token=sha256~3bR5KXgwiUoaQiph2_kIXCDQnVfm_HQy3YwU2m-UOrs --server=https://c109-e.us-east.containers.cloud.ibm.com:31656'\n",
    "OC_LOGIN_COMMAND='oc login --token=sha256~6Xs6va20JZ2CFhS61HN6bpQC2z075XZbhIJt3tZ8L6w --server=https://c109-e.us-east.containers.cloud.ibm.com:31470'\n",
    "$OC_LOGIN_COMMAND\n",
    "oc project postgres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we retrieve the external hostname (or the route or the IP Address) and the port of the Postgres database in the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get the hostname and the port of Postgres\n",
    "oc project postgres\n",
    "externalhostnamepostgres=$(oc get routes | grep nodeport | awk '{print $2}')\n",
    "externalportpostgres=$(oc get svc | grep NodePort | awk '{print $5}' | cut -f2 -d':'| cut -f1 -d'/')\n",
    "echo ------ The following two lines are a hostname '(or IP Address or route)' and a port number --------\n",
    "echo $externalhostnamepostgres\n",
    "echo $externalportpostgres\n",
    "echo ------ paste these values in the part of the code that defines the Postgres connection as instructed below   --------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this data is different from the internal parameters we needed to retrieve in the [Section 7](7_dags_dev.ipynb). At that time, we wanted to bind Airflow with Postgres, which can be done inside the cluster and the external names were not used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to verify that these parameters match the the values displayed in the OpenShift console. The external hostname of Postgres can be found as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/postgres_route.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more steps are necessary in order to see the external port:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/postgres_service_name.png)\n",
    "![](../pictures/postgres_portnr.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is to get the connection parameters for Databand. The hostname (the route in OpenShift's dialect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get the hostname and the port of Databand\n",
    "oc project databand\n",
    "externalhostnamedataband=$(oc get routes | grep databand | awk '{print $2}')\n",
    "\n",
    "echo ------ The following line is the external hostname of databand -------\n",
    "echo $externalhostnamedataband\n",
    "echo ------ paste this value in the part of the code that defines the Databand connection as instructed below   --------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify the name, you can see it in the Openshift console as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/databand_route.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to generate an API key in the Databand user interface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/databand_token1.png)\n",
    "![](../pictures/databand_token2.png)\n",
    "![](../pictures/databand_token3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.  Edit the `pythondag.py` file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to edit the [pythondag.py](../dags/pythondag.py) file. Locate the portion of the code as shown on the picture and replace the values between the double quotes with the parameters collected above. Ensure that the variable `myhost` does not begin with `http://...` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_dag_postgres_conn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to modify other parameters in this section if you didn't change the database name, userid or password."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same file, some lines of code down, the Databand section must be edited analogously but, this time, please ensure that the route of Databand begins with `http://..`:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_dag_databand_conn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file and we are ready to test the program."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Invoke the program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must be located in the directory where the python program is. You may need to modify the cd command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pwd\n",
    "cd ../dags\n",
    "ls -l pythondag.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, you should see something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/Users/Angel/MyIBM/git/databand-workshop/dags\n",
    "-rw-r--r--  1 Angel  wheel  4917 Mar 21 09:03 pythondag.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# This cell runs the pipeline\n",
    "python3 ./pythondag.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output is no longer than two lines:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Databand Tracking Started 1.0.12.3\n",
    "Written: 12 records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual performance data will be displayed on Databand as explained in the sections below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the code structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample python pipeline is described in the following pictures. Notice the cyan blocks, which are the specific code of Databand that we need to add to the program in order to generate performance data that can be displayed in the Databand GUI. You may also see an operation in the Task#3 that will not be logged because it is outside of the monitoring block."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_dag_code1.png)\n",
    "![](../pictures/python_dag_code2.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Display the performance data available in Databand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, you will see a lot of information collected by Databand. Start by selecting the `Python_DAG` pipeline in the project `Python pipelines`,  the names that were given in the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_performance_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why there is no historical information but it is normal because this python pipeline was not scheduled to run periodically, but just ocassionally by hand. Indeed, it was run only two times. However, this was enough to collect the data relevant to the datasets, even grouped by the three different tasks of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_performance_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to take a look at the histogram informoation to find out how the value distribution changes during the task runtime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/python_performance_3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information is good for individual executions. In the next sections, we will focus on displaying historical information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Next Section: [Python on Airflow pipelines](./10_py_air_dag_dev.ipynb) \n",
    "\n",
    "Previous Section: [Python pipelines](./8_SQL_dag_dev.ipynb)\n",
    "\n",
    "[Return to main](../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
