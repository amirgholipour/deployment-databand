{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for DAGs development "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect Airflow and Postgres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing DAGs with Airflow and Postgres, we need to add a connection for Airflow to find the databases. \n",
    "As usual, we login to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the command with your own one inside the single quotes and run the cell\n",
    "# Example OC_LOGIN_COMMAND='oc login --token=sha256~3bR5KXgwiUoaQiph2_kIXCDQnVfm_HQy3YwU2m-UOrs --server=https://c109-e.us-east.containers.cloud.ibm.com:31656'\n",
    "OC_LOGIN_COMMAND='_replace_this_string_by_pasting_the_clipboard_'\n",
    "$OC_LOGIN_COMMAND"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to retrieve two values (the hostname and the port) that we will use immediately. Prepare for copy-and-paste them below in the Airflow new connection menu item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "internalservice=$(oc get svc | grep ClusterIP | awk '{print $1}')\n",
    "internalhostname=$(oc get svc $internalservice -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local')\n",
    "internalport=$(oc get svc | grep ClusterIP | awk '{print $5}' | cut -f1 -d'/')\n",
    "echo Internal hostname of Postgres: $internalhostname\n",
    "echo Internal port of Postgres: $internalport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the connection, we need to access the Airflow admin interface as we did during the **Airflow Deployment** section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflowroute.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-and-paste the values we obtained before in the new connection menu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflow_postgres_conn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## 2. Install the databand monitoring packages "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow will report the pipeline information to Databand and it will be done via the python packages that we will install now. Actually, we already installed Databand packages during the chapter [Airflow integration](./4_airflow_int.ipynb). The following commands are an alternative way that uses a bundled installation syntax and states explicitly the airflow and postgres features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install python package to report Postgres and Airflow information to Databand\n",
    "oc project airflow\n",
    "\n",
    "oc rsh  --shell=/bin/bash airflow-worker-0 /home/airflow/.local/bin/pip install 'databand[airflow,postgres]'\n",
    "POD_SCHEDULER=$(oc get pods | grep airflow-scheduler | awk '{print $1}')\n",
    "oc rsh  --shell=/bin/bash $POD_SCHEDULER /home/airflow/.local/bin/pip install 'databand[airflow,postgres]'\n",
    "\n",
    "echo 'databand[airflow,postgres]'installed in airflow-worker-0 and $POD_SCHEDULER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you would never touch a running container like this to install python packages or additional software in a real production environment. The right way is customizing or extending the docker image as documented [here](https://airflow.apache.org/docs/docker-stack/build.html#extending-vs-customizing-the-image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer of DAGs to Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will transfer some files from our local machine to the Airflow containers. Please ensure that you are in the local directory where the our sample DAGs are located. If you cloned this git repository, the directory is simply called `dags`, under the root level (go up if you are in the jupyter directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# you may need to modify the cd command to place yourself in the DAGs directory\n",
    "pwd\n",
    "cd ../dags\n",
    "ls -l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it right you will see several python file and the `sql`subdirectory. Something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "-rw-r--r--  1 Angel  wheel   152 Mar 13 11:06 databand_airflow_monitor.py\n",
    "-rw-r--r--  1 Angel  wheel  2128 Mar 17 15:57 motogp_dag.py\n",
    "-rw-r--r--  1 Angel  wheel  3371 Mar 17 18:05 pythondag.py\n",
    "-rw-r--r--  1 Angel  wheel  3968 Mar 17 17:48 pythondag_airflow.py\n",
    "drwxr-xr-x  8 Angel  wheel   256 Mar 13 11:06 sql\n",
    "-rw-r--r--  1 Angel  wheel  2110 Mar 17 17:51 sql_airflow_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will transfer some files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc rsh airflow-worker-0  mkdir -p /opt/airflow/dags/sql\n",
    "\n",
    "for file in *.py\n",
    "do \n",
    "  oc cp $file airflow-worker-0:dags/\n",
    "done\n",
    "\n",
    "for file in sql/*\n",
    "do \n",
    "  oc cp $file airflow-worker-0:dags/sql\n",
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is just one of the possibilities to add customized DAGs to Airflow. Other options, some of them more elegant, are documented [here](https://airflow.apache.org/docs/helm-chart/stable/manage-dags-files.html)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Next Section: [Hardware Provisioning](./8_SQL_dag_dev.ipynb)\n",
    "\n",
    "[Return to main](../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
