{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGs development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing DAGs with Airflow and Postgres, we need to add a connection for Airflow to find the databases. \n",
    "As usual, we login to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the command with your own one inside the single quotes and run the cell\n",
    "# Example OC_LOGIN_COMMAND='oc login --token=sha256~3bR5KXgwiUoaQiph2_kIXCDQnVfm_HQy3YwU2m-UOrs --server=https://c109-e.us-east.containers.cloud.ibm.com:31656'\n",
    "OC_LOGIN_COMMAND='_replace_this_string_by_pasting_the_clipboard_'\n",
    "$OC_LOGIN_COMMAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to retrieve two values (the hostname and the port) that we will use immediately. Prepare for copy-and-paste them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "internalservice=$(oc get svc | grep ClusterIP | awk '{print $1}')\n",
    "internalhostname=$(oc get svc $internalservice -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local')\n",
    "internalport=$(oc get svc | grep ClusterIP | awk '{print $5}' | cut -f1 -d'/')\n",
    "echo Internal hostname of Postgres: $internalhostname\n",
    "echo Internal port of Postgres: $internalport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the connection, we need to access the Airflow admin interface as we did during the **Airflow Deployment** section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflowroute.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-and-paste the values we obtained before in the new connection menu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflow_postgres_conn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install the python package to access Postgres\n",
    "oc project airflow\n",
    "oc rsh  --shell=/bin/bash airflow-worker-0 /home/airflow/.local/bin/pip install 'databand[spark,airflow,postgres]'\n",
    "POD_SCHEDULER=$(oc get pods | grep airflow-scheduler | awk '{print $1}')\n",
    "oc rsh  --shell=/bin/bash $POD_SCHEDULER /home/airflow/.local/bin/pip install 'databand[spark,airflow,postgres]'\n",
    "echo 'databand[spark,airflow,postgres]'installed in airflow-worker-0 and $POD_SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc rsh  --shell=/bin/bash airflow-worker-0  mkdir -p /opt/airflow/dags/sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will transfer some files from our local machine to the Airflow containers. Please ensure that you are in the localdirectory of the DAGs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# you may need to modify the cd command to place yourself in the DAGs directory\n",
    "cd ../dags\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it right you will see the `motogp_dag.py`file and the `sql`subdirectory. Something like this"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "-rw-r--r--  1 Angel  wheel   152 Feb 25 12:09 databand_airflow_monitor.py\n",
    "-rw-r--r--  1 Angel  wheel  1690 Mar  9 15:27 motogp_dag.py\n",
    "drwxr-xr-x  8 Angel  wheel   256 Mar  9 13:57 sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will transfer some files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc cp motogp_dag.py airflow-worker-0:dags/ \n",
    "for file in sql/*\n",
    "do \n",
    "  oc cp $file airflow-worker-0:dags/sql\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait one minute or two for Airflow to register the DAG and you can visualize it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/DAG_activate.png)\n",
    "![](../pictures/DAG_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can review the DAG here (do not execute the cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_dag.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Simple DAG for the Databand hands-on workshop\n",
    "\n",
    "# These are mandatory imports\n",
    "from __future__ import annotations\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from datetime import datetime\n",
    "from random import random\n",
    "\n",
    "\n",
    "# An auxiliary function to decide either drop a table or delete its contents\n",
    "def delete_or_drop():\n",
    "    if random() < 0.5:\n",
    "        return(\"motogp_delete_table\")\n",
    "    else:\n",
    "        return(\"motogp_drop_table\")\n",
    "\n",
    "# The main body of the DAG with all its tasks. \n",
    "# Notice the argument postgres_conn_id: it must match a connection name in Airflow\n",
    "with DAG(\n",
    "    dag_id=\"motogp_postgres\",\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    schedule=\"@once\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    motogp_create_table = PostgresOperator (\n",
    "        task_id=\"motogp_create_table\",\n",
    "        postgres_conn_id=\"postgres_motogp\",\n",
    "        sql=\"sql/motogp_create_table.sql\"\n",
    "    )\n",
    "    motogp_load_table = BashOperator (\n",
    "        task_id=\"motogp_load_table\",\n",
    "        bash_command=\"python3 /opt/airflow/dags/sql/motogp_load_table.py\"\n",
    "    )\n",
    "    motogp_select_table = PostgresOperator (\n",
    "        task_id=\"motogp_select_table\",\n",
    "        postgres_conn_id=\"postgres_motogp\",\n",
    "        sql=\"sql/motogp_select_table.sql\"\n",
    "    )\n",
    "    conditional_task = BranchPythonOperator(\n",
    "        task_id=\"delete_or_drop\",\n",
    "        python_callable=delete_or_drop\n",
    "    )\n",
    "    motogp_delete_table = PostgresOperator (\n",
    "        task_id=\"motogp_delete_table\",\n",
    "        postgres_conn_id=\"postgres_motogp\",\n",
    "        sql=\"sql/motogp_delete_table.sql\"\n",
    "    ) \n",
    "    motogp_drop_table = PostgresOperator (\n",
    "        task_id=\"motogp_drop_table\",\n",
    "        postgres_conn_id=\"postgres_motogp\",\n",
    "        sql=\"sql/motogp_drop_table.sql\"\n",
    "    )\n",
    "\n",
    "# These are the DAG dependencies\n",
    "motogp_create_table >> motogp_load_table >> motogp_select_table >> conditional_task\n",
    "conditional_task >> motogp_delete_table\n",
    "conditional_task >> motogp_drop_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_create_table.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "create table \n",
    "if not exists\n",
    "motogp(circuit varchar(40), \n",
    "      class char(6),\n",
    "      constructor varchar(40),\n",
    "      country char(2),\n",
    "      rider varchar(40),\n",
    "      season integer)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_load_table.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import  psycopg2\n",
    "\n",
    "# This code will not be considered as a DAG by Airflow because it is inside a function\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    sql = \"copy motogp from STDIN delimiter ';' csv  header\"\n",
    "\n",
    "    conn = psycopg2.connect(\"dbname='postgres' user='postgres' host='postgresql.postgres.svc.cluster.local' password='postgres'\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    with open(\"/opt/airflow/dags/sql/motogp.csv\", \"r\") as file:\n",
    "        cur.copy_expert(sql, file)\n",
    "    cur.close()\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_select_table.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "select count(*) from motogp\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_delete_table.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "delete from motogp\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp_drop_table.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "drop table motogp\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`motogp.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Circuit;Class;Constructor;Country;Rider;Season\n",
    "Circuit Of The Americas;Moto3;KTM;ES;Jaume Masia;2022\n",
    "Circuit Of The Americas;Moto2;Kalex;IT;Tony Arbolino;2022\n",
    "Circuit Of The Americas;MotoGP;Ducati;IT;Enea Bastianini;2022\n",
    "Termas de Río Hondo;Moto3;GASGAS;ES;Sergio Garcia;2022\n",
    "Termas de Río Hondo;MotoGP;Aprilia;ES;Aleix Espargaro;2022\n",
    "Termas de Río Hondo;Moto2;Kalex;IT;Celestino Vietti;2022\n",
    "Pertamina Mandalika Circuit;MotoGP;KTM;PT;Miguel Oliveira;2022\n",
    "Pertamina Mandalika Circuit;Moto2;Kalex;TH;Somkiat Chantra;2022\n",
    "Pertamina Mandalika Circuit;Moto3;Honda;IT;Dennis Foggia;2022\n",
    "Lusail International Circuit;Moto3;Honda;IT;Andrea Migno;2022\n",
    "Lusail International Circuit;Moto2;Kalex;IT;Celestino Vietti;2022\n",
    "Lusail International Circuit;MotoGP;Ducati;IT;Enea Bastianini;2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
