{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/AirflowLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Airflow deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we login to the OpenShift cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the command with your own one inside the single quotes and run the cell\n",
    "# Example OC_LOGIN_COMMAND='oc login --token=sha256~3bR5KXgwiUoaQiph2_kIXCDQnVfm_HQy3YwU2m-UOrs --server=https://c109-e.us-east.containers.cloud.ibm.com:31656'\n",
    "OC_LOGIN_COMMAND='_replace_this_string_by_pasting_the_clipboard_'\n",
    "$OC_LOGIN_COMMAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We beging by allocating a small piece of storage for our DAGs. We simply call it `my-volume-claim` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# This command creates a small persistent volume claim (1 GB, NFS)\n",
    "\n",
    "oc apply -f - << EOF\n",
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: my-volume-claim\n",
    "  namespace: airflow\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "  storageClassName: managed-nfs-storage\n",
    "  volumeMode: Filesystem\n",
    "status:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reconfigure Airflow to look in our storage to find the DAGs. Additionally, we change one parameter (lazy_load) that is mandatory for the monitoring to work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " helm upgrade --install airflow apache-airflow/airflow \\\n",
    "  --set config.core.lazy_load_plugins=False \\\n",
    "  --set dags.persistence.enabled=true \\\n",
    "  --set dags.persistence.existingClaim=my-volume-claim \\\n",
    "  --set dags.gitSync.enabled=false -f - << EOF\n",
    "env: \n",
    "   - name: AIRFLOW__CORE__LAZY_LOAD_PLUGINS\n",
    "     value: 'False' \n",
    "   - name: _PIP_ADDITIONAL_REQUIREMENTS\n",
    "     value: 'dbnd-airflow-auto-tracking'\n",
    "EOF\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Airflow customization for Databand\n",
    "\n",
    "There are several python libraries that activate specialized monitoring features. Although the previous command installed everything we need, you can optionally install the following additional packages, as you may want to do on a real system:\n",
    "\n",
    "**Warning:** in a production system, you should extend the official container with the package and not install it directly into the pod. For educational purposes, it is OK to modify directly the pod but be aware that these changes will be lost after a redeployment / restart / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install the monitoring package. Expect a long output\n",
    "oc rsh  --shell=/bin/bash airflow-worker-0 /home/airflow/.local/bin/pip install databand 'databand[postgres,airflow]' dbnd-airflow-auto-tracking dbnd-airflow-monitor dbnd-airflow-export dbnd-airflow-versioned-dag\n",
    "POD_SCHEDULER=$(oc get pods | grep airflow-scheduler | awk '{print $1}')\n",
    "oc rsh  --shell=/bin/bash $POD_SCHEDULER /home/airflow/.local/bin/pip install databand 'databand[postgres,airflow]' dbnd-airflow-auto-tracking dbnd-airflow-monitor dbnd-airflow-export dbnd-airflow-versioned-dag\n",
    "echo dbnd-airflow-auto-tracking installed in airflow-worker-0 and $POD_SCHEDULER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell would add a simply DAG that databand needs to initiate the monitors. We copy it into the default directory for the dags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc project airflow\n",
    "echo '# This DAG is used by Databand to monitor your Airflow installation.\n",
    "from airflow_monitor.monitor_as_dag import get_monitor_dag\n",
    "dag = get_monitor_dag()\n",
    "' > databand_airflow_monitor.py\n",
    "\n",
    "oc cp databand_airflow_monitor.py airflow-worker-0:/opt/airflow/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some minutes, you should see a DAG in the Airflow console. Please activate it as indicated in the picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflowmonitor0.png)\n",
    "![](../pictures/airflowmonitor1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this is an auxiliary DAG of databand. Leave it as-is and you may want to experiment with your own ones or simply try a few examples located here https://github.com/apache/airflaow/tree/main/airflow/example_dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl https://raw.githubusercontent.com/apache/airflow/main/airflow/example_dags/example_complex.py > my_test_dag.py\n",
    "curl https://raw.githubusercontent.com/apache/airflow/main/airflow/example_dags/tutorial.py > tutorial.py\n",
    "\n",
    "oc cp my_test_dag.py airflow-worker-0:/opt/airflow/dags\n",
    "oc cp tutorial.py airflow-worker-0:/opt/airflow/dags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/airflowmonitor3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration with databand\n",
    "\n",
    "Now, we will connect Databand to Airflow. Start the Databand console and go to the Integrations secion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the OpenShift console in a separate window and pick the address of the Airflow route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the route of Airflow route in the `Airflow URL` field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the next section as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you you will have to copy-and-paste two fields to create a connection in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Airflow configuration page and the boxes to paste the values picked in the last picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This message indicates that the configuration has been successfuly applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/aircon9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the DAGs examples mentioned before, you need to trigger them manually "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/pipelinetrigger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the two DAGs will be displayed in Databand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pictures/pipelinecheck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Next Section: [Datastage integration](./5_datastage_int.ipynb).   Previous Section: [Airflow deployment](./3_airflow_deploy.ipynb)\n",
    "\n",
    "[Return to main](../README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "vscode": {
   "interpreter": {
    "hash": "7368f8049dce8465172f72d10d47a2dfc42fece53149f0d9b8e5b39df520afe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
